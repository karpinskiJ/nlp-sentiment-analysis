{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\anaconda3\\envs\\kaggle-mirror\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self,vocab_size ,embedding_dim, lstm_hidden_dim, output_dim, dropout_prob=0.2,lstm_layers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim,num_layers=lstm_layers, dropout=dropout_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last time validation loss improved.\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_size = 100000\n",
    "data_path = \"./data/cleaned_data.csv\"\n",
    "df = pd.read_csv(data_path)\\\n",
    "    .dropna()\\\n",
    "     .sample(input_data_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = df.values\n",
    "\n",
    "X = df_values[:, 1].flatten()\n",
    "Y = np.array(df_values[:, 0], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\anaconda3\\envs\\kaggle-mirror\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\karpi\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-roberta-base-sentiment-latest. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\" #\"bert-base-uncased\" # \n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name, padding_side=\"left\") \n",
    "max_len = 64 # 32 max length of the input. everything else will be cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=max_len,        # Maximum length of the sequence\n",
    "        padding='max_length',     # Pad to maximum length\n",
    "        truncation=True,          # Truncate longer sequences\n",
    "        return_tensors='pt'       # Return PyTorch tensors, which are like numpy arrays\n",
    "    )\n",
    "\n",
    "X_toknized = preprocess_text(X.tolist())['input_ids']\n",
    "Y = torch.tensor(Y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_dim = 100\n",
    "output_dim = 2\n",
    "dropout = 0.2\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "weight_decay = 1e-3\n",
    "k_folds = 5\n",
    "learning_rate_decay = 0.1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_toknized, Y, test_size=0.1, stratify=Y, random_state=2)\n",
    "\n",
    "train_dataset = TwitterDataset(X_train, Y_train)\n",
    "test_dataset = TwitterDataset(X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=k_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarpinski-j\u001b[0m (\u001b[33mkarpinski-gsn\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\projects\\nlp-sentiment-analysis\\nlp-sentiment-analysis\\wandb\\run-20250107_230921-996sbumb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karpinski-gsn/nlp-sentiement-analysis/runs/996sbumb' target=\"_blank\">070125230920</a></strong> to <a href='https://wandb.ai/karpinski-gsn/nlp-sentiement-analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karpinski-gsn/nlp-sentiement-analysis' target=\"_blank\">https://wandb.ai/karpinski-gsn/nlp-sentiement-analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karpinski-gsn/nlp-sentiement-analysis/runs/996sbumb' target=\"_blank\">https://wandb.ai/karpinski-gsn/nlp-sentiement-analysis/runs/996sbumb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "run_name = datetime.today().strftime('%d%m%y%H%M%S')\n",
    "wandb_run = wandb.init(\n",
    "    project=\"nlp-sentiement-analysis\",\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"LSTM\",\n",
    "        \"dataset\": \"Twitter Sentiment Analysis\",\n",
    "        \"epochs\": epochs,\n",
    "        \"train_size\": len(train_dataset)*0.8,\n",
    "        \"test_size\": len(train_dataset)*0.2,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"dropout\": dropout,\n",
    "        \"input_data_size\": input_data_size,\n",
    "        \"vocab_size\": tokenizer.vocab_size,\n",
    "        \"token_size\":max_len,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"k_folds\": k_folds,\n",
    "        \"learning_rate_decay\": learning_rate_decay\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########Fold: 1##############\n",
      "#################################################\n",
      "Fold 0 Epoch [1/10], Learing Rate: 0.01, Training Loss: 0.5770  Val Loss: 0.5114, Val Acc: 0.7523, Val Precision: 0.7561, Val Recall: 0.7523, Val F1: 0.7509\n",
      "#################################################\n",
      "Fold 0 Epoch [2/10], Learing Rate: 0.01, Training Loss: 0.5155  Val Loss: 0.5186, Val Acc: 0.7477, Val Precision: 0.7557, Val Recall: 0.7477, Val F1: 0.7462\n",
      "#################################################\n",
      "Fold 0 Epoch [3/10], Learing Rate: 0.01, Training Loss: 0.5043  Val Loss: 0.5057, Val Acc: 0.7552, Val Precision: 0.7580, Val Recall: 0.7552, Val F1: 0.7549\n",
      "#################################################\n",
      "Fold 0 Epoch [4/10], Learing Rate: 0.01, Training Loss: 0.5007  Val Loss: 0.4923, Val Acc: 0.7638, Val Precision: 0.7646, Val Recall: 0.7638, Val F1: 0.7634\n",
      "#################################################\n",
      "Fold 0 Epoch [5/10], Learing Rate: 0.01, Training Loss: 0.4993  Val Loss: 0.4976, Val Acc: 0.7619, Val Precision: 0.7634, Val Recall: 0.7619, Val F1: 0.7613\n",
      "#################################################\n",
      "Fold 0 Epoch [6/10], Learing Rate: 0.01, Training Loss: 0.4989  Val Loss: 0.4912, Val Acc: 0.7645, Val Precision: 0.7652, Val Recall: 0.7645, Val F1: 0.7642\n",
      "#################################################\n",
      "Fold 0 Epoch [7/10], Learing Rate: 0.01, Training Loss: 0.4967  Val Loss: 0.4941, Val Acc: 0.7660, Val Precision: 0.7660, Val Recall: 0.7660, Val F1: 0.7660\n",
      "#################################################\n",
      "Fold 0 Epoch [8/10], Learing Rate: 0.01, Training Loss: 0.4978  Val Loss: 0.4947, Val Acc: 0.7629, Val Precision: 0.7651, Val Recall: 0.7629, Val F1: 0.7626\n",
      "#################################################\n",
      "Fold 0 Epoch [9/10], Learing Rate: 0.01, Training Loss: 0.6614  Val Loss: 0.6635, Val Acc: 0.6153, Val Precision: 0.6276, Val Recall: 0.6153, Val F1: 0.6029\n",
      "#################################################\n",
      "Fold 0 Epoch [10/10], Learing Rate: 0.01, Training Loss: 0.6437  Val Loss: 0.6339, Val Acc: 0.6372, Val Precision: 0.6371, Val Recall: 0.6372, Val F1: 0.6370\n",
      "#########Fold: 2##############\n",
      "#################################################\n",
      "Fold 1 Epoch [1/10], Learing Rate: 0.01, Training Loss: 0.5843  Val Loss: 0.5188, Val Acc: 0.7463, Val Precision: 0.7490, Val Recall: 0.7463, Val F1: 0.7457\n",
      "#################################################\n",
      "Fold 1 Epoch [2/10], Learing Rate: 0.01, Training Loss: 0.5291  Val Loss: 0.5252, Val Acc: 0.7344, Val Precision: 0.7583, Val Recall: 0.7344, Val F1: 0.7281\n",
      "#################################################\n",
      "Fold 1 Epoch [3/10], Learing Rate: 0.01, Training Loss: 0.5150  Val Loss: 0.4987, Val Acc: 0.7644, Val Precision: 0.7650, Val Recall: 0.7644, Val F1: 0.7642\n",
      "#################################################\n",
      "Fold 1 Epoch [4/10], Learing Rate: 0.01, Training Loss: 0.5101  Val Loss: 0.4900, Val Acc: 0.7676, Val Precision: 0.7677, Val Recall: 0.7676, Val F1: 0.7675\n",
      "#################################################\n",
      "Fold 1 Epoch [5/10], Learing Rate: 0.01, Training Loss: 0.5060  Val Loss: 0.4941, Val Acc: 0.7628, Val Precision: 0.7657, Val Recall: 0.7628, Val F1: 0.7621\n",
      "#################################################\n",
      "Fold 1 Epoch [6/10], Learing Rate: 0.01, Training Loss: 0.5075  Val Loss: 0.5008, Val Acc: 0.7556, Val Precision: 0.7638, Val Recall: 0.7556, Val F1: 0.7537\n",
      "#################################################\n",
      "Fold 1 Epoch [7/10], Learing Rate: 0.01, Training Loss: 0.5074  Val Loss: 0.4914, Val Acc: 0.7631, Val Precision: 0.7645, Val Recall: 0.7631, Val F1: 0.7628\n",
      "#################################################\n",
      "Fold 1 Epoch [8/10], Learing Rate: 0.01, Training Loss: 0.5049  Val Loss: 0.4893, Val Acc: 0.7638, Val Precision: 0.7645, Val Recall: 0.7638, Val F1: 0.7637\n",
      "#################################################\n",
      "Fold 1 Epoch [9/10], Learing Rate: 0.01, Training Loss: 0.5037  Val Loss: 0.4846, Val Acc: 0.7671, Val Precision: 0.7684, Val Recall: 0.7671, Val F1: 0.7668\n",
      "#################################################\n",
      "Fold 1 Epoch [10/10], Learing Rate: 0.01, Training Loss: 0.5018  Val Loss: 0.5379, Val Acc: 0.7263, Val Precision: 0.7685, Val Recall: 0.7263, Val F1: 0.7151\n",
      "#########Fold: 3##############\n",
      "#################################################\n",
      "Fold 2 Epoch [1/10], Learing Rate: 0.01, Training Loss: 0.6415  Val Loss: 0.5610, Val Acc: 0.7235, Val Precision: 0.7360, Val Recall: 0.7235, Val F1: 0.7196\n",
      "#################################################\n",
      "Fold 2 Epoch [2/10], Learing Rate: 0.01, Training Loss: 0.5335  Val Loss: 0.5019, Val Acc: 0.7582, Val Precision: 0.7586, Val Recall: 0.7582, Val F1: 0.7581\n",
      "#################################################\n",
      "Fold 2 Epoch [3/10], Learing Rate: 0.01, Training Loss: 0.5089  Val Loss: 0.4914, Val Acc: 0.7634, Val Precision: 0.7634, Val Recall: 0.7634, Val F1: 0.7634\n",
      "#################################################\n",
      "Fold 2 Epoch [4/10], Learing Rate: 0.01, Training Loss: 0.5020  Val Loss: 0.4982, Val Acc: 0.7617, Val Precision: 0.7629, Val Recall: 0.7617, Val F1: 0.7615\n",
      "#################################################\n",
      "Fold 2 Epoch [5/10], Learing Rate: 0.01, Training Loss: 0.5001  Val Loss: 0.4874, Val Acc: 0.7702, Val Precision: 0.7706, Val Recall: 0.7702, Val F1: 0.7700\n",
      "#################################################\n",
      "Fold 2 Epoch [6/10], Learing Rate: 0.01, Training Loss: 0.4984  Val Loss: 0.5006, Val Acc: 0.7563, Val Precision: 0.7628, Val Recall: 0.7563, Val F1: 0.7549\n",
      "#################################################\n",
      "Fold 2 Epoch [7/10], Learing Rate: 0.01, Training Loss: 0.4989  Val Loss: 0.4858, Val Acc: 0.7649, Val Precision: 0.7673, Val Recall: 0.7649, Val F1: 0.7643\n",
      "#################################################\n",
      "Fold 2 Epoch [8/10], Learing Rate: 0.01, Training Loss: 0.4995  Val Loss: 0.4876, Val Acc: 0.7659, Val Precision: 0.7662, Val Recall: 0.7659, Val F1: 0.7659\n",
      "#################################################\n",
      "Fold 2 Epoch [9/10], Learing Rate: 0.01, Training Loss: 0.4984  Val Loss: 0.4976, Val Acc: 0.7592, Val Precision: 0.7679, Val Recall: 0.7592, Val F1: 0.7570\n",
      "#################################################\n",
      "Fold 2 Epoch [10/10], Learing Rate: 0.01, Training Loss: 0.4992  Val Loss: 0.5151, Val Acc: 0.7448, Val Precision: 0.7651, Val Recall: 0.7448, Val F1: 0.7396\n",
      "#########Fold: 4##############\n",
      "#################################################\n",
      "Fold 3 Epoch [1/10], Learing Rate: 0.01, Training Loss: 0.5822  Val Loss: 0.5189, Val Acc: 0.7476, Val Precision: 0.7476, Val Recall: 0.7476, Val F1: 0.7476\n",
      "#################################################\n",
      "Fold 3 Epoch [2/10], Learing Rate: 0.01, Training Loss: 0.5219  Val Loss: 0.5067, Val Acc: 0.7551, Val Precision: 0.7586, Val Recall: 0.7551, Val F1: 0.7542\n",
      "#################################################\n",
      "Fold 3 Epoch [3/10], Learing Rate: 0.01, Training Loss: 0.5061  Val Loss: 0.4910, Val Acc: 0.7649, Val Precision: 0.7656, Val Recall: 0.7649, Val F1: 0.7648\n",
      "#################################################\n",
      "Fold 3 Epoch [4/10], Learing Rate: 0.01, Training Loss: 0.5290  Val Loss: 0.5260, Val Acc: 0.7594, Val Precision: 0.7627, Val Recall: 0.7594, Val F1: 0.7587\n",
      "#################################################\n",
      "Fold 3 Epoch [5/10], Learing Rate: 0.01, Training Loss: 0.5021  Val Loss: 0.4891, Val Acc: 0.7681, Val Precision: 0.7682, Val Recall: 0.7681, Val F1: 0.7680\n",
      "#################################################\n",
      "Fold 3 Epoch [6/10], Learing Rate: 0.01, Training Loss: 0.5006  Val Loss: 0.4860, Val Acc: 0.7675, Val Precision: 0.7675, Val Recall: 0.7675, Val F1: 0.7675\n",
      "#################################################\n",
      "Fold 3 Epoch [7/10], Learing Rate: 0.01, Training Loss: 0.4991  Val Loss: 0.5034, Val Acc: 0.7628, Val Precision: 0.7676, Val Recall: 0.7628, Val F1: 0.7617\n",
      "#################################################\n",
      "Fold 3 Epoch [8/10], Learing Rate: 0.01, Training Loss: 0.5000  Val Loss: 0.4881, Val Acc: 0.7671, Val Precision: 0.7694, Val Recall: 0.7671, Val F1: 0.7666\n",
      "#################################################\n",
      "Fold 3 Epoch [9/10], Learing Rate: 0.01, Training Loss: 0.4983  Val Loss: 0.5024, Val Acc: 0.7594, Val Precision: 0.7664, Val Recall: 0.7594, Val F1: 0.7578\n",
      "#################################################\n",
      "Fold 3 Epoch [10/10], Learing Rate: 0.01, Training Loss: 0.4982  Val Loss: 0.4887, Val Acc: 0.7659, Val Precision: 0.7661, Val Recall: 0.7659, Val F1: 0.7659\n",
      "#########Fold: 5##############\n",
      "#################################################\n",
      "Fold 4 Epoch [1/10], Learing Rate: 0.01, Training Loss: 0.5905  Val Loss: 0.5359, Val Acc: 0.7333, Val Precision: 0.7465, Val Recall: 0.7333, Val F1: 0.7293\n",
      "#################################################\n",
      "Fold 4 Epoch [2/10], Learing Rate: 0.01, Training Loss: 0.5251  Val Loss: 0.5090, Val Acc: 0.7551, Val Precision: 0.7664, Val Recall: 0.7551, Val F1: 0.7528\n",
      "#################################################\n",
      "Fold 4 Epoch [3/10], Learing Rate: 0.01, Training Loss: 0.5087  Val Loss: 0.5062, Val Acc: 0.7541, Val Precision: 0.7654, Val Recall: 0.7541, Val F1: 0.7512\n",
      "#################################################\n",
      "Fold 4 Epoch [4/10], Learing Rate: 0.01, Training Loss: 0.5062  Val Loss: 0.4852, Val Acc: 0.7659, Val Precision: 0.7678, Val Recall: 0.7659, Val F1: 0.7654\n",
      "#################################################\n",
      "Fold 4 Epoch [5/10], Learing Rate: 0.01, Training Loss: 0.5037  Val Loss: 0.4856, Val Acc: 0.7720, Val Precision: 0.7723, Val Recall: 0.7720, Val F1: 0.7719\n",
      "#################################################\n",
      "Fold 4 Epoch [6/10], Learing Rate: 0.01, Training Loss: 0.5013  Val Loss: 0.4801, Val Acc: 0.7721, Val Precision: 0.7722, Val Recall: 0.7721, Val F1: 0.7720\n",
      "#################################################\n",
      "Fold 4 Epoch [7/10], Learing Rate: 0.01, Training Loss: 0.5017  Val Loss: 0.4844, Val Acc: 0.7714, Val Precision: 0.7722, Val Recall: 0.7714, Val F1: 0.7714\n",
      "#################################################\n",
      "Fold 4 Epoch [8/10], Learing Rate: 0.01, Training Loss: 0.4998  Val Loss: 0.4826, Val Acc: 0.7712, Val Precision: 0.7714, Val Recall: 0.7712, Val F1: 0.7712\n",
      "#################################################\n",
      "Fold 4 Epoch [9/10], Learing Rate: 0.01, Training Loss: 0.5009  Val Loss: 0.4821, Val Acc: 0.7717, Val Precision: 0.7738, Val Recall: 0.7717, Val F1: 0.7714\n",
      "#################################################\n",
      "Fold 4 Epoch [10/10], Learing Rate: 0.01, Training Loss: 0.5016  Val Loss: 0.4808, Val Acc: 0.7713, Val Precision: 0.7713, Val Recall: 0.7713, Val F1: 0.7713\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_fold_0</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch_fold_1</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch_fold_2</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch_fold_3</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>epoch_fold_4</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy_fold_0</td><td>▆███████▁▃</td></tr><tr><td>train_accuracy_fold_1</td><td>▁▆▇▇██▇███</td></tr><tr><td>train_accuracy_fold_2</td><td>▁▇████████</td></tr><tr><td>train_accuracy_fold_3</td><td>▁▆█▅██████</td></tr><tr><td>train_accuracy_fold_4</td><td>▁▇████████</td></tr><tr><td>train_loss_fold_0</td><td>▄▂▁▁▁▁▁▁█▇</td></tr><tr><td>train_loss_fold_1</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_fold_2</td><td>█▃▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_fold_3</td><td>█▃▂▄▁▁▁▁▁▁</td></tr><tr><td>train_loss_fold_4</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy_fold_0</td><td>▇▇▇█████▁▂</td></tr><tr><td>val_accuracy_fold_1</td><td>▄▂▇█▇▆▇▇█▁</td></tr><tr><td>val_accuracy_fold_2</td><td>▁▆▇▇█▆▇▇▆▄</td></tr><tr><td>val_accuracy_fold_3</td><td>▁▄▇▅██▆█▅▇</td></tr><tr><td>val_accuracy_fold_4</td><td>▁▅▅▇██████</td></tr><tr><td>val_f1_fold_0</td><td>▇▇██████▁▂</td></tr><tr><td>val_f1_fold_1</td><td>▅▃██▇▆▇▇█▁</td></tr><tr><td>val_f1_fold_2</td><td>▁▆▇▇█▆▇▇▆▄</td></tr><tr><td>val_f1_fold_3</td><td>▁▃▇▅██▆█▅▇</td></tr><tr><td>val_f1_fold_4</td><td>▁▅▅▇██████</td></tr><tr><td>val_loss_fold_0</td><td>▂▂▂▁▁▁▁▁█▇</td></tr><tr><td>val_loss_fold_1</td><td>▅▆▃▂▂▃▂▂▁█</td></tr><tr><td>val_loss_fold_2</td><td>█▃▂▂▁▂▁▁▂▄</td></tr><tr><td>val_loss_fold_3</td><td>▇▅▂█▂▁▄▁▄▁</td></tr><tr><td>val_loss_fold_4</td><td>█▅▄▂▂▁▂▁▁▁</td></tr><tr><td>val_precision_fold_0</td><td>▇▇██████▁▁</td></tr><tr><td>val_precision_fold_1</td><td>▁▄▇█▇▆▇▇██</td></tr><tr><td>val_precision_fold_2</td><td>▁▆▇▆█▆▇▇▇▇</td></tr><tr><td>val_precision_fold_3</td><td>▁▅▇▆█▇▇█▇▇</td></tr><tr><td>val_precision_fold_4</td><td>▁▆▆▆███▇█▇</td></tr><tr><td>val_recall_fold_0</td><td>▇▇▇█████▁▂</td></tr><tr><td>val_recall_fold_1</td><td>▄▂▇█▇▆▇▇█▁</td></tr><tr><td>val_recall_fold_2</td><td>▁▆▇▇█▆▇▇▆▄</td></tr><tr><td>val_recall_fold_3</td><td>▁▄▇▅██▆█▅▇</td></tr><tr><td>val_recall_fold_4</td><td>▁▅▅▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_fold_0</td><td>9</td></tr><tr><td>epoch_fold_1</td><td>9</td></tr><tr><td>epoch_fold_2</td><td>9</td></tr><tr><td>epoch_fold_3</td><td>9</td></tr><tr><td>epoch_fold_4</td><td>9</td></tr><tr><td>train_accuracy_fold_0</td><td>0.64087</td></tr><tr><td>train_accuracy_fold_1</td><td>0.76446</td></tr><tr><td>train_accuracy_fold_2</td><td>0.7661</td></tr><tr><td>train_accuracy_fold_3</td><td>0.76769</td></tr><tr><td>train_accuracy_fold_4</td><td>0.76383</td></tr><tr><td>train_loss_fold_0</td><td>0.64366</td></tr><tr><td>train_loss_fold_1</td><td>0.50179</td></tr><tr><td>train_loss_fold_2</td><td>0.49919</td></tr><tr><td>train_loss_fold_3</td><td>0.49822</td></tr><tr><td>train_loss_fold_4</td><td>0.5016</td></tr><tr><td>val_accuracy_fold_0</td><td>0.63717</td></tr><tr><td>val_accuracy_fold_1</td><td>0.72633</td></tr><tr><td>val_accuracy_fold_2</td><td>0.74483</td></tr><tr><td>val_accuracy_fold_3</td><td>0.76589</td></tr><tr><td>val_accuracy_fold_4</td><td>0.77133</td></tr><tr><td>val_f1_fold_0</td><td>0.63701</td></tr><tr><td>val_f1_fold_1</td><td>0.71506</td></tr><tr><td>val_f1_fold_2</td><td>0.73961</td></tr><tr><td>val_f1_fold_3</td><td>0.76585</td></tr><tr><td>val_f1_fold_4</td><td>0.77132</td></tr><tr><td>val_loss_fold_0</td><td>0.63393</td></tr><tr><td>val_loss_fold_1</td><td>0.53789</td></tr><tr><td>val_loss_fold_2</td><td>0.51512</td></tr><tr><td>val_loss_fold_3</td><td>0.48866</td></tr><tr><td>val_loss_fold_4</td><td>0.48076</td></tr><tr><td>val_precision_fold_0</td><td>0.63709</td></tr><tr><td>val_precision_fold_1</td><td>0.7685</td></tr><tr><td>val_precision_fold_2</td><td>0.76508</td></tr><tr><td>val_precision_fold_3</td><td>0.7661</td></tr><tr><td>val_precision_fold_4</td><td>0.77134</td></tr><tr><td>val_recall_fold_0</td><td>0.63717</td></tr><tr><td>val_recall_fold_1</td><td>0.72633</td></tr><tr><td>val_recall_fold_2</td><td>0.74483</td></tr><tr><td>val_recall_fold_3</td><td>0.76589</td></tr><tr><td>val_recall_fold_4</td><td>0.77133</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">070125230920</strong> at: <a href='https://wandb.ai/karpinski-gsn/nlp-sentiement-analysis/runs/996sbumb' target=\"_blank\">https://wandb.ai/karpinski-gsn/nlp-sentiement-analysis/runs/996sbumb</a><br> View project at: <a href='https://wandb.ai/karpinski-gsn/nlp-sentiement-analysis' target=\"_blank\">https://wandb.ai/karpinski-gsn/nlp-sentiement-analysis</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250107_230921-996sbumb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "models_path = f\"./models/{run_name}\"\n",
    "if not os.path.exists(models_path):\n",
    "    os.makedirs(models_path)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset)):\n",
    "    ############ MODEL ###############\n",
    "    models = []\n",
    "    model = LSTMModel(tokenizer.vocab_size,embedding_dim, hidden_dim, output_dim, dropout).to(device)  \n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=learning_rate_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    ####### EARLY STOPPING ########\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=4, min_delta=0.01)\n",
    "\n",
    "    fold_val_loss = []\n",
    "    fold_val_acc = []\n",
    "    fold_val_precision = []\n",
    "    fold_val_recall = []\n",
    "    fold_val_f1 = []\n",
    "\n",
    "    print(f'#########Fold: {fold+1}##############')\n",
    "    #### SPLIT DATASET ####\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    val_subset = Subset(train_dataset, val_indices)\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    train_samples = len(train_subset)\n",
    "    val_samples = len(val_subset)\n",
    "\n",
    "    #### TRAINING LOOP ####\n",
    "    for epoch in range(epochs):\n",
    "        best_val_loss = float('inf')\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        #### EPOCH TRAINING LOOP ####\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_total += labels.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            ### EPCH METIRCS ###     \n",
    "        train_loss_avg = train_loss / train_total\n",
    "        train_acc_avg = train_correct / train_total\n",
    "\n",
    "\n",
    "        ### EPOCH VALIDATION LOOP ###\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        \n",
    "            val_loss_avg = val_loss / val_samples\n",
    "            val_acc_avg = val_correct / val_total\n",
    "            val_precision = precision_score(all_labels, all_predictions,average='weighted')\n",
    "            val_recall = recall_score(all_labels, all_predictions,average='weighted')\n",
    "            val_f1 = f1_score(all_labels, all_predictions,average='weighted')\n",
    "\n",
    "        #Save the model with the best validation loss\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            best_model = model.state_dict()\n",
    "  \n",
    "        ### PRINT EPOCH METRICS ###\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(\"#################################################\")\n",
    "        print(f'Fold {fold} Epoch [{epoch+1}/{epochs}], Learing Rate: {lr}, Training Loss: {train_loss_avg:.4f}  Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc_avg:.4f}, Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        logs = {\n",
    "            f\"epoch_fold_{fold}\": epoch,\n",
    "            f\"train_loss_fold_{fold}\": train_loss_avg,\n",
    "            f\"train_accuracy_fold_{fold}\": train_acc_avg,\n",
    "            f\"val_loss_fold_{fold}\": val_loss_avg,\n",
    "            f\"val_accuracy_fold_{fold}\": val_acc_avg,\n",
    "            f\"val_precision_fold_{fold}\": val_precision,\n",
    "            f\"val_recall_fold_{fold}\": val_recall,\n",
    "            f\"val_f1_fold_{fold}\": val_f1\n",
    "        }\n",
    "        wandb_run.log(logs)\n",
    "\n",
    "        best_model_name = f\"model_fold_{fold}.pth\"    \n",
    "        best_model_path = f\"{models_path}/{best_model_name}.pth\"  \n",
    "        if early_stopping(val_loss_avg):\n",
    "            print(f'#######Early stopping on epoch {epoch}################')\n",
    "            torch.save(best_model, best_model_path)\n",
    "            artifact = wandb.Artifact(best_model_name, type='model')\n",
    "            artifact.add_file(best_model_path)\n",
    "            wandb_run.log_artifact(artifact)\n",
    "            break\n",
    "\n",
    "    best_model_name = f\"model_fold_{fold}\"    \n",
    "    best_model_path = f\"{models_path}/{best_model_name}.pth\"        \n",
    "    torch.save(best_model, best_model_path)\n",
    "    artifact = wandb.Artifact(best_model_name, type='model')\n",
    "    artifact.add_file(best_model_path)\n",
    "    wandb_run.log_artifact(artifact)\n",
    "\n",
    "wandb_run.finish()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karpi\\AppData\\Local\\Temp\\ipykernel_12052\\4224380926.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_1.load_state_dict(torch.load(f\"{models_path}/model_fold_0.pth\"))\n",
      "C:\\Users\\karpi\\AppData\\Local\\Temp\\ipykernel_12052\\4224380926.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_2.load_state_dict(torch.load(f\"{models_path}/model_fold_1.pth\"))\n",
      "C:\\Users\\karpi\\AppData\\Local\\Temp\\ipykernel_12052\\4224380926.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_3.load_state_dict(torch.load(f\"{models_path}/model_fold_2.pth\"))\n",
      "C:\\Users\\karpi\\AppData\\Local\\Temp\\ipykernel_12052\\4224380926.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_4.load_state_dict(torch.load(f\"{models_path}/model_fold_3.pth\"))\n",
      "C:\\Users\\karpi\\AppData\\Local\\Temp\\ipykernel_12052\\4224380926.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_5.load_state_dict(torch.load(f\"{models_path}/model_fold_4.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embedding): Embedding(50265, 128)\n",
       "  (lstm): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (fc): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_1 = LSTMModel(tokenizer.vocab_size,embedding_dim, hidden_dim, output_dim, dropout).to(device)\n",
    "model_2 = LSTMModel(tokenizer.vocab_size,embedding_dim, hidden_dim, output_dim, dropout).to(device)\n",
    "model_3 = LSTMModel(tokenizer.vocab_size,embedding_dim, hidden_dim, output_dim, dropout).to(device)\n",
    "model_4 = LSTMModel(tokenizer.vocab_size,embedding_dim, hidden_dim, output_dim, dropout).to(device)\n",
    "model_5 = LSTMModel(tokenizer.vocab_size,embedding_dim, hidden_dim, output_dim, dropout).to(device)\n",
    "\n",
    "model_1.load_state_dict(torch.load(f\"{models_path}/model_fold_0.pth\"))\n",
    "model_2.load_state_dict(torch.load(f\"{models_path}/model_fold_1.pth\"))\n",
    "model_3.load_state_dict(torch.load(f\"{models_path}/model_fold_2.pth\"))\n",
    "model_4.load_state_dict(torch.load(f\"{models_path}/model_fold_3.pth\"))\n",
    "model_5.load_state_dict(torch.load(f\"{models_path}/model_fold_4.pth\"))\n",
    "\n",
    "model_1.eval()\n",
    "model_2.eval()\n",
    "model_3.eval()\n",
    "model_4.eval()\n",
    "model_5.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_labels = []\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "results = []\n",
    "for model in [model_1, model_2, model_3, model_4, model_5]:\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "\n",
    "        val_acc_avg = val_correct / val_total\n",
    "        val_precision = precision_score(all_labels, all_predictions,average='weighted')\n",
    "        val_recall = recall_score(all_labels, all_predictions,average='weighted')\n",
    "        val_f1 = f1_score(all_labels, all_predictions,average='weighted')\n",
    "        results.append({\n",
    "            \"val_acc_avg\":f\"{val_acc_avg:.4f}\",\n",
    "            \"val_precision\": f\"{val_precision:.4f}\",\n",
    "            \"val_recall\":f\"{val_recall:.4f}\",\n",
    "            \"val_f1\": f\"{val_f1:.4f}\"\n",
    "        }\n",
    "            \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_acc_avg': '0.7093', 'val_precision': '0.6473', 'val_recall': '0.6473', 'val_f1': '0.6472'}\n",
      "{'val_acc_avg': '0.7206', 'val_precision': '0.7808', 'val_recall': '0.7431', 'val_f1': '0.7337'}\n",
      "{'val_acc_avg': '0.7333', 'val_precision': '0.7873', 'val_recall': '0.7714', 'val_f1': '0.7680'}\n",
      "{'val_acc_avg': '0.7452', 'val_precision': '0.7930', 'val_recall': '0.7926', 'val_f1': '0.7926'}\n",
      "{'val_acc_avg': '0.7495', 'val_precision': '0.7713', 'val_recall': '0.7713', 'val_f1': '0.7713'}\n"
     ]
    }
   ],
   "source": [
    "for results in results:\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User input test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\" #TODO FILL ME \n",
    "embedding = preprocess_text(text)['input_ids'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_1(embedding)\n",
    "_, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet '' has positive sentiment\n"
     ]
    }
   ],
   "source": [
    "if predicted:\n",
    "    print(f\"Tweet '{text}' has positive sentiment\")\n",
    "else:\n",
    "    print(f\"Tweet '{text}' has negative sentiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-mirror",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
